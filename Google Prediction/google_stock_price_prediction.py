# -*- coding: utf-8 -*-
"""Google stock price prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E4J1PDQ9SSG6bKQwXoVGXWYqARlG1LPf

In this project, I will be using 4 **LSTM** layers with **Bidirectional** layers, 4 dropout layers, a **Fully Connected** layer and a **dense** layer in the end to predict the high price.

LSTM layers will have 64 units each and dropout layer will have drop probability of 0.1
"""

# Commented out IPython magic to ensure Python compatibility.
# importing all the required modules
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf
from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout
from tensorflow.keras.optimizers import Adam

import math
from sklearn.metrics import mean_squared_error

# uploading the dataset, I won't be using the date column as index for the simplicity
dataset = pd.read_csv("GOOG.csv")
dataset.info()

# checking the shape of the dataset
dataset.shape

# plotting the "High" value from the whole dataset
dataset["High"].plot(figsize = (10, 4))
plt.xlabel("No. of the Day")
plt.ylabel("Amount")
plt.title("Google Stock Price")
plt.grid(True)
plt.show()

# plotting this graph to decide from where to split the dataset into train and test
dataset["High"][4500 : ].plot(figsize = (10, 4))
plt.xlabel("No. of the Day")
plt.ylabel("Amount")
plt.title("Google Stock Price")
plt.grid(True)
plt.show()

# splitting the whole dataset into train and test dataset
train_data = dataset[ : 4600].iloc[:, 2:3].values
test_data = dataset[4600 : ].iloc[:, 2:3].values

# checking the shape of the train and test dataset
print(f"Shape of training data : {train_data.shape}")
print(f"Shape of the test dataset : {test_data.shape}")

# plotting the whole dataset with train and test data in diiferent colours
plt.figure(figsize = (10, 4))
plt.plot(dataset["High"][: 4600])
plt.plot(dataset["High"][4600 :])
plt.legend(["Training Dataset", "Test Dataset"])
plt.xlabel("No. of the Day")
plt.ylabel("Amount")
plt.title("Google Stock Price")
plt.grid(True)
plt.show()

# using MinMaxScaler to scale the input values between 0 and 1
sc = MinMaxScaler(feature_range=(0,1))

training_set_scaled = sc.fit_transform(train_data)

x_train = []
y_train = []

# making the windows of length 60
for i in range(60, train_data.shape[0]):
  x_train.append(training_set_scaled[i-60: i, 0])
  y_train.append(training_set_scaled[i, 0])

# changing the list to a nparray for easy training of the model.
x_train, y_train = np.array(x_train, dtype = "float"), np.array(y_train, dtype = "float")

# checking the shape of both x_train and y_train.
print(f"Shape of x_train : {x_train.shape}")
print(f"Shape of y_train : {y_train.shape}")

# reshaping the train dataset into 3 dimension, as the LSTM model accepts 3 dimensional values only
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
print(f"Shape of x_train : {x_train.shape}")

# making the model using TensorFlow Functional API
class MyModel(tf.keras.Model):

  def __init__(self, lstm_units = None, dropout_value = None):
    super().__init__()

    # keeping the return_sequence = True, with this each LSTM unit will return the values instead of last unit only.
    # LSTM layer 1
    self.lstm_1 = Bidirectional(LSTM(units = lstm_units, return_sequences = True))
    self.dropout_1 = Dropout(dropout_value)

    # LSTM layer 2
    self.lstm_2 = Bidirectional(LSTM(units = lstm_units, return_sequences = True))
    self.dropout_2 = Dropout(dropout_value)

    # LSTM layer 3
    self.lstm_3 = Bidirectional(LSTM(units = lstm_units, return_sequences = True))
    self.dropout_3 = Dropout(dropout_value)

    # LSTM layer 4
    self.lstm_4 = Bidirectional(LSTM(units = lstm_units))
    self.dropout_4 = Dropout(dropout_value)

    # Fully connected layer
    self.dense_1 = Dense(units = 15)

    # output layer
    self.output_1 = Dense(1)

  def call(self, inputs):

    x = self.lstm_1(inputs)
    x = self.dropout_1(x)

    x = self.lstm_2(x)
    x = self.dropout_2(x)

    x = self.lstm_3(x)
    x = self.dropout_3(x)

    x = self.lstm_4(x)
    x = self.dropout_4(x)

    x = self.dense_1(x)

    x = self.output_1(x)

    return x

# defining the model
model = MyModel(lstm_units = 64, dropout_value = 0.2)

# using Adam optimizer with learning rate set to 0.001, beta_1 = 0.9, 0.998 and epsilon as 1e-7
model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001,
                                                   beta_1 = 0.9,
                                                   beta_2 = 0.998,
                                                   epsilon = 1e-7),
              loss = tf.keras.losses.MeanSquaredError())

# training the model and setitng the epochs to 25
model.fit(x_train,
          y_train,
          epochs = 25)

# information about different layers of the model
model.summary()

# scaling the values of test dataset between 0 and 1
testing_set_scaled = sc.fit_transform(test_data)

x_test = []
y_test = []

for i in range(60, test_data.shape[0]):
  x_test.append(testing_set_scaled[i-60: i, 0])
  y_test.append(testing_set_scaled[i, 0])

x_test, y_test = np.array(x_test, dtype = "float"), np.array(y_test, dtype = "float")

print(f"Shape of x_test : {x_test.shape}")
print(f"Shape of y_test : {y_test.shape}")

# reshaping x_test into a 3 dimension array
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))
print(f"Shape of x_test : {x_test.shape}")

# predicting the values of x_test using the model
prediction = model.predict(x_test)

# as we have used 60 as a window which will make the shape of prediction less compared to the original shape in the test_dataset
test_data_plot = test_data = dataset[4660 : ].iloc[:, 2:3].values

# checking the shape to make sure test_data and prediction is of same shape
print(f"Shape of test_data for plotting : {test_data_plot.shape}")
print(f"Shape of the prediction data : {prediction.shape}")

# reversing the scaling that is done on the data
prediction = sc.inverse_transform(prediction)

# plotting a graph of original data compared to predicted data
plt.plot(test_data_plot, label = "Original Price")
plt.plot(prediction, label = "Predicted Price")
plt.title("Original Price vs. Predicted Price")
plt.ylabel("Price of the stock")
plt.grid(True)
plt.legend()
plt.show()

# saving the model in a pickle file
import pickle

model_pkl_file = "Google_model.pkl"
with open(model_pkl_file, "wb") as file:
  pickle.dump(model, file)